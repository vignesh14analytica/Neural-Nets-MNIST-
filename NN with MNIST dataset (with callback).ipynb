{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MNIST dataset from Keras\n",
    "\n",
    "mnist = tf.keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset into training and test sets\n",
    "\n",
    "(training_images, training_labels),(test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing data from the training images\n",
    "\n",
    "plt.imshow(training_images[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalising the training and test set images\n",
    "\n",
    "training_images = training_images / 255\n",
    "test_images = test_images / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring the model with one hidden layer\n",
    "# The output layer is a softmax layer\n",
    "\n",
    "model = tf.keras.Sequential([tf.keras.layers.Flatten(),\n",
    "                             tf.keras.layers.Dense(units = 1024, activation = tf.nn.relu),\n",
    "                             tf.keras.layers.Dense(units = 10, activation = tf.nn.softmax)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model with Adam Optimizer and the lost function\n",
    "\n",
    "model.compile(optimizer = tf.optimizers.Adam(), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model for 5 epochs\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the model summary\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the test set accuracy\n",
    "\n",
    "model.evaluate(test_images, test_labels)\n",
    "\n",
    "# an accuracy of 87% is achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting images in the test set\n",
    "\n",
    "classifications = model.predict(test_images)\n",
    "\n",
    "print(classifications[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introducing call back to stop model when required state is achieved while training \n",
    "\n",
    "# callback is created here when loss < 0.4\n",
    "import tensorflow as tf\n",
    "\n",
    "# creating the class for callback functionality\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs = {}):\n",
    "        if(logs.get('accuracy') > 0.6): #The model should terminate when the accuracy is > 60%\n",
    "            print(\"\\nReached 60% accuracy so cancelling training!\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "# Using MNIST data from keras\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# Instantiating the callback class\n",
    "callbacks = myCallback()\n",
    "\n",
    "# Loading the data into training set and test set\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalising the training and test data\n",
    "x_test = x_test / 255\n",
    "x_train = x_train / 255\n",
    "\n",
    "# Initiating the training model with one hidden layer\n",
    "model = tf.keras.Sequential([tf.keras.layers.Flatten(), # Flattening the input layer\n",
    "                             tf.keras.layers.Dense(units = 128, activation = tf.nn.relu),\n",
    "                             tf.keras.layers.Dense(units = 10, activation = tf.nn.softmax)])\n",
    "\n",
    "# Compiling model with Adam optimizer and loss function is sparse_categorical_crossentropy\n",
    "model.compile(tf.optimizers.Adam(), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Training the model. The model stops at epoch one as the accuracy is 92% and callback is enabled. \n",
    "model.fit(x_train, y_train, epochs = 10, callbacks = [callbacks])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
